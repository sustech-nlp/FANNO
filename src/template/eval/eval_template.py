
# def recent_eval(instruction):
#     DOC_EVALUATE_PROMPT = """I want you act as a instruction evaluator. Please evaluate this instruction and respond with '0' (bad) or '1' (good), without giving reasons.

# Standard: A good instruction Must not involve recent or current events. Historical events are fine.

# Example1:
# Instruction: Please analyze the recent COVID-19 outbreak.
# Answer: 0 (Reason: recent)

# Example2:
# Instruction: What's happening in China in September 2023?
# Answer: 0 (Reason: in September 2023)

# Example3:
# Instruction: Provide an account of events from last Monday night.
# Answer: 0 (Reason: last Monday night)

# ### Instruction:
# {instruction}

# ### Answer:"""
#     return DOC_EVALUATE_PROMPT.format(instruction=instruction)


# def privacy_eval(instruction):
#     DOC_EVALUATE_PROMPT = """I want you act as a instruction evaluator. Please evaluate this instruction and respond with '0' (bad) or '1' (good), without giving reasons.

# Standard: A good instruction must not include any private information like names, addresses, phone numbers, etc, unless the person is historical or famous.

# Example1:
# Instruction: What is the name of the person who lives at 123 Main Street?
# Answer: 0 (Reason: private information)

# Example2:
# Instruction: What is the name of the first president of the United States?
# Answer: 1 (Reason: historical)

# Example3:
# Instruction: What is the address of the CEO of Microsoft?
# Answer: 0 (Reason: private information)


# ### Instruction:
# {instruction}

# ###Answer:"""
#     return DOC_EVALUATE_PROMPT.format(instruction=instruction)



# def privacy_eval(instruction):
#     DOC_EVALUATE_PROMPT = """Please evaluate this instruction for the inclusion of personal information such as names, addresses, phone numbers, etc., unless the person is historical or famous.
# Only respond with '0' (Don't include personal information) or '1' (Include personal information).

# ### Instruction:
# {instruction}

# ### Answer:"""
#     return DOC_EVALUATE_PROMPT.format(instruction=instruction)

# def answerable_eval(instruction):
#     DOC_EVALUATE_PROMPT = """I want you act as a instruction evaluator. Please evaluate this instruction and respond with '0' (bad) or '1' (good), without giving reasons.

# Standard: A good instruction is perfectly logical, practical, and can be fully understood by a human.
# A bad instruction, likely generated by AI, is generally vague, weird, complex and long. It may seem to string unrelated words, topics, and tasks together.

# Example1: 
# Instruction: Considering the health benefits of a non-dairy diet, how does the emotional response of individuals vary when they attend social events where dairy-based foods are served?
# Answer: 0

# Example2:
# Instruction: Create a multidisciplinary essay that explores the and historical origins of the dish 'Shrimp Alfredo Pasta Bake'. Discuss the various ingredients, their origins. Additionally, translate the recipe instructions from English to Spanish.
# Answer: 0


# ### Instruction:
# {instruction}

# ### Answer:"""
#     return DOC_EVALUATE_PROMPT.format(instruction=instruction)


from typing import List, Dict
# vll
def privacy_eval(instructions: List[str]) -> List[str]:
    prompt_template = (
        "Evaluate the following instruction to determine if it contains the name of an unfamiliar person, place, or organization. "
        "If it doesn't, score it as 1. If it does, score it as 0. Output the score in the format: "
        "`score: X`, where X is either 0 or 1.\n\n"
        "Instruction: \"{}\"\n\nOutput:"
    )
    return prompt_template.format(instructions)


# 1 is good 0 is bad 
def safety_eval(instructions: List[str]) -> List[str]:
    prompt_template = (
        "Evaluate the following instruction to determine if it is safe, which means it doesn't contain any dangerous, discriminatory, or harmful content. "
        "If it is safe, score it as 1. If it is not, score it as 0. Output the score in the format: "
        "`score: X`, where X is either 0 or 1.\n\n"
        "Instruction: \"{}\"\n\nOutput:"
    )
    return prompt_template.format(instructions)


# 1 is good 0 is bad
def originality_eval(instructions: List[str]) -> List[str]:
    prompt_template = (
        "Evaluate the following instruction to determine if it is original, interesting, and special, rather than similar "
        "to common questions starting with 'how', 'what', 'which', etc. If it is original, interesting, and special, score it as 1. "
        "If it is similar to common questions, score it as 0. Output the score in the format: `score: X`, where X is either 0 or 1.\n\n"
        "Instruction: \"{}\"\n\nOutput:"
    )
    return prompt_template.format(instructions)

# 1 is good 0 is bad 
def difficult_eval(instructions: List[str]) -> List[str]:
    prompt_template = (
        "Evaluate the following instruction to determine if it is difficult to solve and requires several steps of logical reasoning, or advanced knowledge from graduate-level courses. If it is difficult, score it as 1. "
        "If it is easy to understand or execute, score it as 0. Output the score in the format: `score: X`, where X is either 0 or 1.\n\n" 
        "Example 1, the instruction 'Write a report on the contributions of Dr. Naliah Kareem to molecular biology' is difficult to solve and requires several steps of logical reasoning.\n"
        "Example 2, the instruction 'How do you make a cake?' is easy to understand or execute.\n"
        "Example 3, the instruction 'Write a code to print the first 10 prime numbers.' is easy to understand or execute.\n"
        "Example 4, the instruction 'Write a paper about the history of the United States.' is easy to understand or execute.\n"
        "Example 5, the instruction 'How to solve the Twin prime conjecture' is difficult to solve and requires several steps of logical reasoning.\n"
        "Instruction: \"{}\"\n\nOutput:"
    )
    
    return prompt_template.format(instructions)



def insjudge_eval(instructions: List[str]) -> List[str]:
    prompt_template = (
        "Evaluate the following instruction to determine if it is a question or a command, rather than an article or a paragraph. "
        "If it is a question or a command, score it as 1. If it is an article or a paragraph, score it as 0. Output the score in the format: "
        "`score: X`, where X is either 0 or 1.\n\n"
        "Instruction: \"{}\"\n\nOutput:"
    )
    return prompt_template.format(instructions) 


# =======================================Pick up======================================================

# copy and modify from Humpbac
def faithfulness_eval(instruction, response):
    DOC_EVALUATE_PROMPT = """Below is an instruction from an user and a candidate answer. 
Let's think step by step.
Evaluate whether or not the answer is a good example of how AI Assistant should respond to the user's instruction. Please assign a score using the following 5-point scale:
1: It means the answer is incomplete, vague, off-topic, or not exactly what the user asked for. For example, some content seems missing. Or the response is from another personâ€™s perspective with their personal experience (e.g. taken from blog posts). Or it contains promotional text or other irrelevant information.
2: (between 1 and 3)
3: It means the answer is helpful but not written by an AI Assistant. It addresses all the basic asks from the user. It is complete and self contained with the drawback that the response is not written from an AI assistant's perspective, but from other people's perspective. For example, it contains personal experience or opinion, mentions comments section, or share on social media, etc.
4: (between 3 and 5)
5: It means it is a perfect answer from an AI Assistant. It has a clear focus on being a helpful AI Assistant, where the response looks like intentionally written to address the user's question or instruction without any irrelevant sentences. The answer provides high quality content, demonstrating expert knowledge in the area, is very well written, logical, easy-to-follow, engaging and insightful.

Your reply should be only 1 or 2 or 3 or 4 or 5, without providing any reasoning and explanation.

###Instruction:
{instruction}

###Answer:
{response}

###Your Reply:"""
    return DOC_EVALUATE_PROMPT.format(instruction=instruction, response=response)



